{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import flax.linen as nn\n",
    "from typing import Sequence\n",
    "import einx\n",
    "import optax\n",
    "from jax import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PATH = \"training_data/labels.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get input output matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open(TRAINING_PATH) as f:\n",
    "\tfor i in f:\n",
    "\t\tlines.append(i)\n",
    "\n",
    "n_sequences = len(lines)//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>d2bkma | 128\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-HHHHH--HHHHHHHHHHHHHHHHH----HHHH----HHHHHHHHHHHHHHHH----------HHHHH------HHHHHHHHHHHHHHHHHH---HHHHHHHHHHHHHHHHHH---\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(n_sequences):\n",
    "\tseq_id,seq_len = (lines[i*3]).split(\" | \")\n",
    "\tassert seq_id[0] == \">\"\n",
    "\tseq_id = seq_id[1:]\n",
    "\tseq_len = int(seq_len)\n",
    "\tseq = lines[i*3+1].strip()\n",
    "\tlabels = lines[i*3+2].strip()\n",
    "\n",
    "\tsequences.append([seq_id,seq,labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5326"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5185, dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.argmax(jnp.array([len(i[1]) for i in sequences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = jnp.array([len(i[1]) for i in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([116, 128, 147, ..., 111, 124, 119], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.07341345, dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lengths>384).sum()/len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Count'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d3elka',\n",
       " 'TRERILHGLITLYILKELVKRPHGYELQKSFETTGQALPQGSIYILLKTKERGFVISESSVNKGQQLTVYHITDAGKKFLDHSQALQLARKIIDDLLSTVD',\n",
       " '---HHHHHHHHHHHHHHHHH---HHHHHHHHH---------HHHHHHHHHHH---------------------HHHHHHH----HHHHHHHHHHHHHH---']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
    "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
    "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
    "\n",
    "amino_acids = list(d.values())\n",
    "\n",
    "amino_to_int_dict = {i:idx+3 for idx,i in enumerate(amino_acids)}\n",
    "amino_to_int_dict[\"<PAD>\"] = 0\n",
    "amino_to_int_dict[\"<MASK>\"] = 1\n",
    "amino_to_int_dict[\"<END>\"] = 0\n",
    "int_to_amino_dict = {v:k for v,k in amino_to_int_dict.items()}\n",
    "\n",
    "secondary_to_int_dict = {\"<PAD>\":2,\"-\":0,\"H\":1,\"<END>\":0}\n",
    "int_to_secondary_dict = {v:k for v,k in secondary_to_int_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_from_dict(sequence,tokenizer_dict,length=None):\n",
    "\n",
    "\tif length is None:\n",
    "\t\tlength = len(sequence)\n",
    "\n",
    "\tseq = []\n",
    "\n",
    "\tfor i in range(length):\n",
    "\n",
    "\t\tif i>len(sequence):\n",
    "\t\t\tseq.append(tokenizer_dict[\"<PAD>\"])\n",
    "\t\telif i == len(sequence):\n",
    "\t\t\tseq.append(tokenizer_dict[\"<END>\"])\n",
    "\t\telse:\n",
    "\t\t\tseq.append(tokenizer_dict[sequence[i]])\n",
    "\n",
    "\treturn jnp.array(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "l_max = 384\n",
    "\n",
    "for seq in sequences:\n",
    "\tif len(seq[1])<=l_max-1:\n",
    "\t\tinputs.append(tokenize_from_dict(seq[1],amino_to_int_dict,l_max))\n",
    "\t\tlabels.append(tokenize_from_dict(seq[2],secondary_to_int_dict,l_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(sequences,l_max):\n",
    "\n",
    "\tinputs = []\n",
    "\tlabels = []\n",
    "\n",
    "\tfor seq in sequences:\n",
    "\t\tif len(seq[1])<=l_max-1:\n",
    "\t\t\tinputs.append(tokenize_from_dict(seq[1],amino_to_int_dict,l_max))\n",
    "\t\t\tlabels.append(tokenize_from_dict(seq[2],secondary_to_int_dict,l_max))\n",
    "\n",
    "\treturn jnp.stack(inputs),jnp.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = get_xy(sequences,384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4934, 384)\n",
      "(4934, 384)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizedConv(nn.Module):\n",
    "\tfeature:int\n",
    "\tkernel_size:int\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self,x):\n",
    "\t\tx = nn.Dense(self.feature)(x)\n",
    "\t\tx = nn.Conv(self.feature,self.kernel_size,feature_group_count=self.feature)(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\tfeatures: Sequence[int]\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self,x):\n",
    "\t\tfor feature in self.features[:-1]:\n",
    "\t\t\tx = nn.relu(FactorizedConv(feature,3)(x))\n",
    "\t\tx = FactorizedConv(self.features[-1],3)(x)\n",
    "\t\tx = nn.LayerNorm()(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_pos_encode(x):\n",
    "\tembeddings = jnp.zeros((x.shape))\n",
    "\tbatch,length,dim = x.shape\n",
    "\tposition = jnp.arange(0,length)\n",
    "\tomega = jnp.exp(jnp.arange(0,dim,2) * math.log(10000)/dim)\n",
    "\tembeddings = embeddings.at[:,:,0::2].set(jnp.sin(einx.multiply(\"l,d -> l d\",position,omega)))\n",
    "\tembeddings = embeddings.at[:,:,1::2].set(jnp.cos(einx.multiply(\"l,d -> l d\",position,omega)))\n",
    "\n",
    "\treturn embeddings + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560000"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMHA(nn.Module):\n",
    "\thidden_dim:int\n",
    "\theads:int\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self,x,mask):\n",
    "\n",
    "\t\tq = nn.Dense(self.hidden_dim)(x)\n",
    "\n",
    "\t\tq = q * ((q.shape[-1] // self.heads) ** -0.5)\n",
    "\t\tattn = einx.dot(\"b q (h c), b k (h c) -> b q k h\", q, x, h=self.heads)\n",
    "\n",
    "\t\tattn = einx.where(\"b k, b q k h,\", mask, attn, -jnp.inf)\n",
    "\n",
    "\t\tattn = einx.softmax(\"b q [k] h\", attn)\n",
    "\t\ty = einx.dot(\"b q k h, b k (h c) -> b q (h c)\", attn, x)\n",
    "\n",
    "\t\treturn x + y\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\thidden_dim:int\n",
    "\theads:int\n",
    "\texpansion_factor:int\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self,x,mask):\n",
    "\n",
    "\t\t#x = x + nn.LayerNorm()(MaskedMHA(hidden_dim=self.hidden_dim,heads=self.heads)(x,mask))\n",
    "\t\tx = x + nn.LayerNorm()(MLP([self.hidden_dim*self.expansion_factor,self.hidden_dim])(x))\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\tn_amino:int\n",
    "\tamino_dim:int\n",
    "\tn_heads:int\n",
    "\tn_layers:int\n",
    "\texpansion_factor:int\n",
    "\n",
    "\t@nn.compact\n",
    "\tdef __call__(self,x,mask):\n",
    "\t\tx = nn.Embed(self.n_amino,self.amino_dim)(x)\n",
    "\t\tx = x + sin_pos_encode(x)\n",
    "\n",
    "\t\tfor _ in range(self.n_layers):\n",
    "\t\t\tx = EncoderBlock(self.amino_dim,self.n_heads,self.expansion_factor)(x,mask)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLM(nn.Module):\n",
    "\tn_amino:int\n",
    "\tamino_dim:int\n",
    "\tn_heads:int\n",
    "\tn_layers:int\n",
    "\texpansion_factor:int\n",
    "\n",
    "\tdef setup(self):\n",
    "\n",
    "\t\tself.transformer = Transformer(n_amino=self.n_amino,\n",
    "\t\t\t\t\t\t\t\t amino_dim=self.amino_dim,\n",
    "\t\t\t\t\t\t\t\t n_heads=self.n_heads,\n",
    "\t\t\t\t\t\t\t\t n_layers=self.n_layers,\n",
    "\t\t\t\t\t\t\t\t expansion_factor=self.expansion_factor)\n",
    "\t\t\n",
    "\t\tself.layer_norm = nn.LayerNorm()\n",
    "\t\tself.masked_lm_head = nn.Dense(self.n_amino)\n",
    "\t\tself.structure_head = nn.Dense(2)\n",
    "\n",
    "\tdef __call__(self,x,mask):\n",
    "\n",
    "\t\tx = self.transformer(x,mask)\n",
    "\t\tx = self.layer_norm(x)\n",
    "\t\treturn x\n",
    "\t\n",
    "\tdef lm_forward(self,x,mask):\n",
    "\t\tx = self(x,mask)\n",
    "\t\treturn self.masked_lm_head(x)\n",
    "\t\n",
    "\tdef structure_forward(self,x,mask):\n",
    "\t\tx = self(x,mask)\n",
    "\t\treturn self.structure_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_from_list(x,values):\n",
    "\n",
    "\tmask = jnp.zeros_like(x,dtype=jnp.bool)\n",
    "\n",
    "\tfor i in values:\n",
    "\t\tmask += (x == i)\n",
    "\n",
    "\treturn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amino_to_int_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedLM(n_amino=len(amino_to_int_dict),\n",
    "\t\t\t\t\tamino_dim=32,\n",
    "\t\t\t\t\tn_heads=2,\n",
    "\t\t\t\t\tn_layers=6,\n",
    "\t\t\t\t\texpansion_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = jnp.ones((16,256),dtype=jnp.int32)\n",
    "model_var = model.init(key,batch,mask_from_list(batch,[0]),method=\"structure_forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                               MaskedLM Summary                                \u001b[0m\n",
      "┌───────────────┬──────────────┬───────────────┬──────────────┬───────────────┐\n",
      "│\u001b[1m \u001b[0m\u001b[1mpath         \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mmodule      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1moutputs     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mparams       \u001b[0m\u001b[1m \u001b[0m│\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│               │ MaskedLM     │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mint32\u001b[0m[2,384]  │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer   │ Transformer  │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mint32\u001b[0m[2,384]  │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Embed        │ \u001b[2mint32\u001b[0m[2,384]  │ \u001b[2mfloat32\u001b[0m[2,3… │ embedding:    │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[23,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m736 \u001b[0m\u001b[1;2m(2.9 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ EncoderBlock │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mfloat32\u001b[0m[2,38… │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ MLP          │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32,6… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,112 \u001b[0m\u001b[1;2m(8.4 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,080 \u001b[0m\u001b[1;2m(8.3 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ EncoderBlock │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mfloat32\u001b[0m[2,38… │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ MLP          │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32,6… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,112 \u001b[0m\u001b[1;2m(8.4 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,080 \u001b[0m\u001b[1;2m(8.3 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ EncoderBlock │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mfloat32\u001b[0m[2,38… │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ MLP          │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32,6… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,112 \u001b[0m\u001b[1;2m(8.4 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,080 \u001b[0m\u001b[1;2m(8.3 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ EncoderBlock │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mfloat32\u001b[0m[2,38… │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ MLP          │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32,6… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,112 \u001b[0m\u001b[1;2m(8.4 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,080 \u001b[0m\u001b[1;2m(8.3 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ EncoderBlock │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mfloat32\u001b[0m[2,38… │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ MLP          │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32,6… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,112 \u001b[0m\u001b[1;2m(8.4 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,080 \u001b[0m\u001b[1;2m(8.3 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ EncoderBlock │ -             │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "│               │              │ \u001b[2mfloat32\u001b[0m[2,38… │              │               │\n",
      "│               │              │ - \u001b[2mbool\u001b[0m[2,384] │              │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ MLP          │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32,6… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,112 \u001b[0m\u001b[1;2m(8.4 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m256 \u001b[0m\u001b[1;2m(1.0 KB)\u001b[0m  │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ FactorizedC… │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │               │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Dense        │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[64,3… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m2,080 \u001b[0m\u001b[1;2m(8.3 \u001b[0m   │\n",
      "│               │              │               │              │ \u001b[1;2mKB)\u001b[0m           │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ Conv         │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ kernel:       │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[3,1,… │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m128 \u001b[0m\u001b[1;2m(512 B)\u001b[0m   │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ transformer/… │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│ layer_norm    │ LayerNorm    │ \u001b[2mfloat32\u001b[0m[2,38… │ \u001b[2mfloat32\u001b[0m[2,3… │ bias:         │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │ scale:        │\n",
      "│               │              │               │              │ \u001b[2mfloat32\u001b[0m[32]   │\n",
      "│               │              │               │              │               │\n",
      "│               │              │               │              │ \u001b[1m64 \u001b[0m\u001b[1;2m(256 B)\u001b[0m    │\n",
      "├───────────────┼──────────────┼───────────────┼──────────────┼───────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m29,024 \u001b[0m\u001b[1;2m(116.1\u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m               \u001b[0m│\u001b[1m              \u001b[0m│\u001b[1m               \u001b[0m│\u001b[1m              \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mKB)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\n",
      "└───────────────┴──────────────┴───────────────┴──────────────┴───────────────┘\n",
      "\u001b[1m                                                                               \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 29,024 \u001b[0m\u001b[1;2m(116.1 KB)\u001b[0m\u001b[1m                      \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nn.tabulate(model,key)(inputs[0:2],mask_from_list(inputs[0:2],[0,1,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4934, 384)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4934, 384)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_cross_entropy_loss(logits, labels, mask):\n",
    "    loss = optax.softmax_cross_entropy(logits, one_hot(labels, logits.shape[-1]))\n",
    "    masked_loss = loss * mask\n",
    "    return jnp.sum(masked_loss) / jnp.sum(mask)\n",
    "\n",
    "def one_hot(x, num_classes):\n",
    "    return jnp.eye(num_classes)[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inputs[0:5]\n",
    "y = labels[0:5]\n",
    "mask = mask_from_list(x,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(vars,x,mask,y,pred_mask):\n",
    "\n",
    "\ty_pred = model.apply(vars,x,mask,method=MaskedLM.structure_forward)\n",
    "\tloss = masked_cross_entropy_loss(y_pred,y,pred_mask)\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 384, 2)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(model_var,x,mask,method=\"structure_forward\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7075600028038025"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model_var,x,mask,y,jnp.logical_not(mask)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_accuracy(logits, labels, mask):\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    correct_predictions = (predictions == labels) * mask\n",
    "    accuracy = jnp.sum(correct_predictions) / jnp.sum(mask)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train(model,\n",
    "\t\t\t\t key,\n",
    "\t\t\t\t model_var,\n",
    "\t\t\t\t x,\n",
    "\t\t\t\t y,\n",
    "\t\t\t\t batch_size,\n",
    "\t\t\t\t forward,\n",
    "\t\t\t\t n_epochs):\n",
    "\t\n",
    "\tdef loss_fn(vars,x,mask,y,pred_mask):\n",
    "\n",
    "\t\ty_pred = model.apply(vars,x,mask,method=MaskedLM.structure_forward)\n",
    "\t\tloss = masked_cross_entropy_loss(y_pred,y,pred_mask)\n",
    "\t\tacc = masked_accuracy(y_pred,y,pred_mask)\n",
    "\t\treturn loss,(acc)\n",
    "\t\n",
    "\toptimizer = optax.adam(1E-3)\n",
    "\topt_state = optimizer.init(model_var)\n",
    "\tloss_grad_fn = jax.value_and_grad(loss_fn,has_aux=True)\n",
    "\tloss_grad_fn = jax.jit(loss_grad_fn)\n",
    "\n",
    "\tfor epoch in range(n_epochs):\n",
    "\n",
    "\t\tpermutations = random.permutation(key,jnp.arange(len(x)))\n",
    "\t\tpermutations = permutations[0:batch_size*(len(x)//batch_size)]\n",
    "\t\tbatches = einx.rearrange(\"(n b) -> n b\",permutations,b=batch_size)\n",
    "\n",
    "\t\tlosses = []\n",
    "\n",
    "\t\tfor b,indices in enumerate(batches):\n",
    "\n",
    "\t\t\tx_batch = x[indices]\n",
    "\t\t\ty_batch = y[indices]\n",
    "\n",
    "\t\t\tpad_mask = mask_from_list(x_batch,[0,1])\n",
    "\n",
    "\t\t\tmasked = random.uniform(key,x_batch.shape) > 0.9\n",
    "\t\t\tmasked = jnp.logical_and(masked,jnp.logical_not(pad_mask))\n",
    "\n",
    "\t\t\t(loss,(acc)),grad = loss_grad_fn(model_var,x_batch,pad_mask,y_batch,masked)\n",
    "\n",
    "\t\t\tupdates,opt_state = optimizer.update(grad,opt_state)\n",
    "\t\t\tmodel_var = optax.apply_updates(model_var,updates)\n",
    "\t\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t\tif b % 10 == 0:\n",
    "\t\t\t\tprint(acc)\n",
    "\n",
    "\t\tprint(np.mean(losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42541435\n",
      "0.56122446\n",
      "0.6231648\n",
      "0.6101695\n",
      "0.60600704\n",
      "0.6003788\n",
      "0.57385397\n",
      "0.60847455\n",
      "0.6630934\n",
      "0.63205826\n",
      "0.6378378\n",
      "0.6606607\n",
      "0.7138211\n",
      "0.627907\n",
      "0.68485916\n",
      "0.7207703\n",
      "0.6443745946729338\n",
      "0.72007364\n",
      "0.75\n",
      "0.7161501\n",
      "0.7016949\n",
      "0.6819788\n",
      "0.7462121\n",
      "0.68930393\n",
      "0.73050845\n",
      "0.67381316\n",
      "0.7140255\n",
      "0.7279279\n",
      "0.6996997\n",
      "0.7398374\n",
      "0.6976744\n",
      "0.7253521\n",
      "0.738652\n",
      "0.5548894178944749\n",
      "0.7495396\n",
      "0.77040815\n",
      "0.7471452\n",
      "0.75084746\n",
      "0.69787985\n",
      "0.7613636\n",
      "0.69949067\n",
      "0.73220336\n",
      "0.68147016\n",
      "0.72313297\n",
      "0.73333335\n",
      "0.7117117\n",
      "0.75609756\n",
      "0.71594685\n",
      "0.7429578\n",
      "0.74965614\n",
      "0.539179409479166\n",
      "0.74401474\n",
      "0.7823129\n",
      "0.75203913\n",
      "0.75932205\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[432], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcustom_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[43mmodel_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[43mMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructure_forward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m\t\t\t \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[431], line 43\u001b[0m, in \u001b[0;36mcustom_train\u001b[1;34m(model, key, model_var, x, y, batch_size, forward, n_epochs)\u001b[0m\n\u001b[0;32m     40\u001b[0m (loss,(acc)),grad \u001b[38;5;241m=\u001b[39m loss_grad_fn(model_var,x_batch,pad_mask,y_batch,masked)\n\u001b[0;32m     42\u001b[0m updates,opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grad,opt_state)\n\u001b[1;32m---> 43\u001b[0m model_var \u001b[38;5;241m=\u001b[39m \u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_updates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rahar\\anaconda3\\envs\\neuroai\\Lib\\site-packages\\optax\\_src\\update.py:42\u001b[0m, in \u001b[0;36mapply_updates\u001b[1;34m(params, updates)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_updates\u001b[39m(params: base\u001b[38;5;241m.\u001b[39mParams, updates: base\u001b[38;5;241m.\u001b[39mUpdates) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m base\u001b[38;5;241m.\u001b[39mParams:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies an update to the corresponding parameters.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m  This is a utility functions that applies an update to a set of parameters, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Updated parameters, with same structure, shape and type as `params`.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdates\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahar\\anaconda3\\envs\\neuroai\\Lib\\site-packages\\jax\\_src\\tree_util.py:344\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[0;32m    342\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m    343\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahar\\anaconda3\\envs\\neuroai\\Lib\\site-packages\\jax\\_src\\tree_util.py:344\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    342\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[0;32m    343\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[1;32mc:\\Users\\rahar\\anaconda3\\envs\\neuroai\\Lib\\site-packages\\optax\\_src\\update.py:43\u001b[0m, in \u001b[0;36mapply_updates.<locals>.<lambda>\u001b[1;34m(p, u)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_updates\u001b[39m(params: base\u001b[38;5;241m.\u001b[39mParams, updates: base\u001b[38;5;241m.\u001b[39mUpdates) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m base\u001b[38;5;241m.\u001b[39mParams:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies an update to the corresponding parameters.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m  This is a utility functions that applies an update to a set of parameters, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Updated parameters, with same structure, shape and type as `params`.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(\n\u001b[1;32m---> 43\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m p, u: jnp\u001b[38;5;241m.\u001b[39masarray(\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m)\u001b[38;5;241m.\u001b[39mastype(jnp\u001b[38;5;241m.\u001b[39masarray(p)\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[0;32m     44\u001b[0m       params, updates)\n",
      "File \u001b[1;32mc:\\Users\\rahar\\anaconda3\\envs\\neuroai\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[1;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[1;32mc:\\Users\\rahar\\anaconda3\\envs\\neuroai\\Lib\\site-packages\\jax\\_src\\numpy\\ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[1;34m(self, out, where, *args)\u001b[0m\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[1;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "custom_train(model,\n",
    "\t\t\t key,\n",
    "\t\t\t model_var,\n",
    "\t\t\t inputs,\n",
    "\t\t\t labels,\n",
    "\t\t\t 32,\n",
    "\t\t\t MaskedLM.structure_forward,\n",
    "\t\t\t 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[False, False, False, ...,  True, False, False],\n",
       "       [False, False, False, ...,  True, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False,  True, ..., False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.uniform(key,x.shape)>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first have to worry about processing\n",
    "1. We need to get the data into lists of tuples (sequence,length,labels)\n",
    "2. We tokenize the sequences and binarize the labels\n",
    "3. We construct (tokenized) input/output sequences with maximum length\n",
    "4. Probably select a cutoff length\n",
    "   - let's say 512\n",
    "\n",
    "- Once we have the inputs and output pairs we can move into training\n",
    "- Implementing the neural network is trivial and independent of the task\n",
    "- Afterwards, there are two ways of training the neural network\n",
    "  - direct supervised training\n",
    "  - masked language modeling - as you would with AlphaFold,ESMFold,... - followed by supervised training\n",
    "  - this is a cute blog post\n",
    "\n",
    "- There are \"only\" like 5000 sequences which may be somewhat small for deep-learning applications (unclear with masked modeling?)\n",
    "  - in any case, we may want to perform data augmentation by randomly switching similar amino-acids together?\n",
    "  - the other option would be to reduce the dictionnary size?\n",
    "    - I feel like having augmentations would have the same effect as restricting the dictionnary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
